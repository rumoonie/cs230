{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48da353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusing imports from baseline as well\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from astropy.io import fits\n",
    "from reproject import reproject_interp\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "# adding\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L, models as M\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small unet ; input will be multichannel, \n",
    "# chandra counts + chandra/xmm exposure\n",
    "\n",
    "def build_unet(input_shape=(128, 128, 2)):\n",
    "    inputs = L.Input(shape=input_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    c1 = L.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    c1 = L.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(c1)\n",
    "    p1 = L.MaxPool2D(2)(c1)\n",
    "\n",
    "    c2 = L.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(p1)\n",
    "    c2 = L.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(c2)\n",
    "    p2 = L.MaxPool2D(2)(c2)\n",
    "\n",
    "    c3 = L.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(p2)\n",
    "    c3 = L.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(c3)\n",
    "    p3 = L.MaxPool2D(2)(c3)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = L.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(p3)\n",
    "    b = L.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(b)\n",
    "\n",
    "    # Upsampling\n",
    "    u3 = L.UpSampling2D(2)(b)\n",
    "    u3 = L.Concatenate()([u3, c3])\n",
    "    c4 = L.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(u3)\n",
    "    c4 = L.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(c4)\n",
    "\n",
    "    u2 = L.UpSampling2D(2)(c4)\n",
    "    u2 = L.Concatenate()([u2, c2])\n",
    "    c5 = L.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(u2)\n",
    "    c5 = L.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(c5)\n",
    "\n",
    "    u1 = L.UpSampling2D(2)(c5)\n",
    "    u1 = L.Concatenate()([u1, c1])\n",
    "    c6 = L.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(u1)\n",
    "    c6 = L.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(c6)\n",
    "\n",
    "    outputs = L.Conv2D(1, 1, padding=\"same\", activation=\"linear\")(c6)\n",
    "    model = M.Model(inputs, outputs, name=\"unet_chandra_to_xmm\")\n",
    "    return model\n",
    "\n",
    "unet = build_unet()\n",
    "unet.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training patches/tf.data w aligned reproj_ch+img reproj_ch_exp xmm_img from baseline \n",
    "# use here\n",
    "PATCH = 128\n",
    "BATCH = 4\n",
    "\n",
    "def extract_patches(ch_img, ch_exp, xmm_img, n_patches=256, patch_size=PATCH):\n",
    "    H, W = ch_img.shape\n",
    "    X_list, Y_list = [], []\n",
    "\n",
    "    for _ in range(n_patches):\n",
    "        i = np.random.randint(0, H - patch_size)\n",
    "        j = np.random.randint(0, W - patch_size)\n",
    "\n",
    "        ch_patch  = ch_img[i:i+patch_size, j:j+patch_size]\n",
    "        exp_patch = ch_exp[i:i+patch_size, j:j+patch_size]\n",
    "        xmm_patch = xmm_img[i:i+patch_size, j:j+patch_size]\n",
    "\n",
    "        # stack channels: [Chandra counts, Chandra exp]\n",
    "        inp = np.stack([ch_patch, exp_patch], axis=-1)  # (H,W,2)\n",
    "        X_list.append(inp.astype(np.float32))\n",
    "        Y_list.append(xmm_patch[..., None].astype(np.float32))  # (H,W,1)\n",
    "\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    Y = np.stack(Y_list, axis=0)\n",
    "    return X, Y\n",
    "\n",
    "X_all, Y_all = extract_patches(reproj_ch_img, reproj_ch_exp, xmm_img,\n",
    "                               n_patches=512, patch_size=PATCH)\n",
    "\n",
    "print(\"All patches:\", X_all.shape, Y_all.shape)\n",
    "\n",
    "# simple train/val split\n",
    "N = X_all.shape[0]\n",
    "split = int(0.8 * N)\n",
    "X_train, X_val = X_all[:split], X_all[split:]\n",
    "Y_train, Y_val = Y_all[:split], Y_all[split:]\n",
    "\n",
    "print(\"Train:\", X_train.shape, Y_train.shape)\n",
    "print(\"Val:\",   X_val.shape,   Y_val.shape)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "train_ds = train_ds.shuffle(512).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
    "val_ds = val_ds.batch(BATCH).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ab7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physics loss reconstruction+flux; l1 reocnstruction (counts), flux term of absolute diff of flux per patch, ssim term\n",
    "def flux_loss(y_true, y_pred):\n",
    "    # total flux per patch\n",
    "    true_flux = tf.reduce_sum(y_true, axis=[1,2,3])\n",
    "    pred_flux = tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    return tf.reduce_mean(tf.abs(true_flux - pred_flux))\n",
    "\n",
    "def recon_loss_l1(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "def ssim_loss(y_true, y_pred):\n",
    "    maxv = tf.reduce_max(y_true) + 1e-6\n",
    "    ssim = tf.image.ssim(y_true, y_pred, max_val=maxv)\n",
    "    return 1.0 - tf.reduce_mean(ssim)  # minimize 1-SSIM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aee601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined\n",
    "lambda_flux = 1.0\n",
    "lambda_ssim = 0.1  # small\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    l_recon = recon_loss_l1(y_true, y_pred)\n",
    "    l_flux  = flux_loss(y_true, y_pred)\n",
    "    l_ssim  = ssim_loss(y_true, y_pred)\n",
    "    return l_recon + lambda_flux * l_flux + lambda_ssim * l_ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ablation\n",
    "#lambda_flux = 0.0\n",
    "#lambda_ssim = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55cfd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training step\n",
    "optimizer = tf.keras.optimizers.Adam(2e-4)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = unet(x, training=True)\n",
    "        loss = total_loss(y, y_pred)\n",
    "    grads = tape.gradient(loss, unet.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, unet.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b28a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "EPOCHS = 5\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # training\n",
    "    train_losses = []\n",
    "    for x_batch, y_batch in train_ds:\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        train_losses.append(loss.numpy())\n",
    "    train_loss = float(np.mean(train_losses))\n",
    "\n",
    "    # validation\n",
    "    val_losses = []\n",
    "    for x_batch, y_batch in val_ds:\n",
    "        y_pred = unet(x_batch, training=False)\n",
    "        val_loss = total_loss(y_batch, y_pred).numpy()\n",
    "        val_losses.append(val_loss)\n",
    "    val_loss_mean = float(np.mean(val_losses))\n",
    "\n",
    "    train_history.append(train_loss)\n",
    "    val_history.append(val_loss_mean)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - train: {train_loss:.4f}  val: {val_loss_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc2fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "#plt.plot(train_history, label=\"train\")\n",
    "#plt.plot(val_history, label=\"val\")\n",
    "#plt.xlabel(\"Epoch\")\n",
    "#plt.ylabel(\"Loss\")\n",
    "#plt.legend()\n",
    "#plt.title(\"U-Net training vs validation loss\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one consistent patch region from the *original* image to compare\n",
    "i0, j0 = 100, 100  # choose something inside \n",
    "patch = PATCH\n",
    "\n",
    "# extract for comparison\n",
    "ch_patch  = reproj_ch_img[i0:i0+patch, j0:j0+patch]\n",
    "exp_patch = reproj_ch_exp[i0:i0+patch, j0:j0+patch]\n",
    "xmm_patch = xmm_img[i0:i0+patch, j0:j0+patch]\n",
    "\n",
    "# baseline on patch\n",
    "baseline_patch = forward_model_baseline(\n",
    "    ch_patch, exp_patch, xmm_exp[i0:i0+patch, j0:j0+patch],\n",
    "    sigma_px=4.0, poisson_scale=100.0, add_poisson=False\n",
    ")\n",
    "\n",
    "# unet prediction on that patch\n",
    "inp = np.stack([ch_patch, exp_patch], axis=-1)[None, ...].astype(np.float32) # (1,H,W,2)\n",
    "unet_pred = unet(inp, training=False).numpy()[0, ..., 0]  # (H,W)\n",
    "\n",
    "# metrics (baseline compute_metrics)\n",
    "psnr_b, ssim_b, flux_t, flux_b, ferr_b = compute_metrics(baseline_patch, xmm_patch)\n",
    "psnr_u, ssim_u, _,      flux_u, ferr_u = compute_metrics(unet_pred,      xmm_patch)\n",
    "\n",
    "print(\"Baseline vs XMM:\")\n",
    "print(f\"  PSNR: {psnr_b:.3f} dB, SSIM: {ssim_b:.3f}, flux err: {ferr_b:.2%}\")\n",
    "print(\"U-Net vs XMM:\")\n",
    "print(f\"  PSNR: {psnr_u:.3f} dB, SSIM: {ssim_u:.3f}, flux err: {ferr_u:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6caa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vmin = np.percentile(xmm_patch, 5)\n",
    "#vmax = np.percentile(xmm_patch, 99)\n",
    "\n",
    "#fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "#for a in ax: a.set_axis_off()\n",
    "\n",
    "#ax[0].imshow(xmm_patch, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "#ax[0].set_title(\"True XMM patch\")\n",
    "\n",
    "#ax[1].imshow(baseline_patch, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "#ax[1].set_title(\"Baseline\")\n",
    "\n",
    "#ax[2].imshow(unet_pred, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "#ax[2].set_title(\"U-Net\")\n",
    "\n",
    "#resid_unet = xmm_patch - unet_pred\n",
    "#im = ax[3].imshow(resid_unet, origin=\"lower\")\n",
    "#ax[3].set_title(\"Residual (XMM - U-Net)\")\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
